{
    "batch_size": 64,
    "encode": "one-hot",
    "epochs": 100,
    "early_stop": 10,
    "lr": 0.001,
    "convolution_layers": {
        "n_layers": 1,
        "filters": [128],
        "kernel_sizes": [7]
    },
    "transformer_layers": {
        "n_layers": 6,
        "attn_key_dim": [16, 16, 16, 32, 32, 32],
        "attn_heads": [4, 4, 4, 8, 8, 8]
    },
    "n_dense_layer": 1,
    "dense_neurons1": 64,
    "dropout_conv": "yes",
    "dropout_prob": 0.4,
    "pad": "same"
}
